This project involves cleaning a dataset sourced from GitHub, consisting of 2800+ entries detailing layoffs across various companies during a specific period. The data cleaning process ensures the dataset is accurate, consistent, and ready for analysis.

#Process of Data cleaning
#---1. Remove Duplicates
#---2. Standardize the data (Like spelling mistakes)
#---3. Null values or blank values
#---4. Remove unnecessary columns and rows

#---Enhanced Data Cleaning Steps in SQL
1. Identify and Remove Duplicate Entries:
Duplicate records can distort analysis. By finding and removing duplicates, I ensure the dataset accurately reflects the unique instances of layoffs, providing a clearer picture for analysis.

2. Standardize Data Formats:
Ensuring uniform data representation is crucial. I standardize formats and correct typos to maintain consistency, making it easier to analyze and compare the data.

3. Handle Missing Values:
Addressing null or blank values is vital to maintain data integrity. I ensure that any missing information is either appropriately filled or the incomplete records are removed to avoid misleading analysis results.

4. Remove Unnecessary Data:
Streamlining the dataset by eliminating irrelevant columns and rows helps focus on the essential data, improving the efficiency and effectiveness of the analysis.
